{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T08:42:57.178137Z",
     "start_time": "2020-09-26T08:42:57.171913Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd                \n",
    "import glob, os\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              AdaBoostClassifier,\n",
    "                              ExtraTreesClassifier,\n",
    "                              StackingClassifier)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (StratifiedKFold,\n",
    "                                    RandomizedSearchCV,\n",
    "                                    cross_val_score,\n",
    "                                    train_test_split)\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from myhelper import (extract_column_names,\n",
    "                      data_prep,\n",
    "                      sampling,\n",
    "                      ColumnSelector)\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "from myconfig import (TARGET_COL,\n",
    "                      SAMPLE_SIZE,\n",
    "                      SEED)  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T12:18:16.919627Z",
     "start_time": "2020-09-25T12:18:16.892143Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pipeline(model, col_filter, col_to_scale, power_tf=False, scaler_tf=False, **kwargs):\n",
    "    \n",
    "    steps = []\n",
    "    transformers = []\n",
    "\n",
    "    # Adding dataset column selector\n",
    "    steps.append(('col_selector', ColumnSelector(col_filter)))\n",
    "    # adding power transformer\n",
    "    if power_tf and len(col_to_scale)>0:\n",
    "        transformers.append(('power_tf', PowerTransformer(method='yeo-johnson', standardize=False), col_to_scale))\n",
    "    # adding scaler transformer\n",
    "    if scaler_tf and len(col_to_scale)>0:\n",
    "        transformers.append(('data_tf', StandardScaler(), col_to_scale))\n",
    "    # adding transformer to pipeline definition\n",
    "    if len(transformers)>0:\n",
    "        steps.append(('data_tf', ColumnTransformer(transformers=transformers, remainder='passthrough')))\n",
    "    # adding model\n",
    "    steps.append(('clf', model))\n",
    "    # generate pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def process_pipeline(X, y, pipeline, space, max_evals):\n",
    "    \n",
    "    def objective(params):\n",
    "        # define stratified cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "        # set pipeline params\n",
    "        pipeline.set_params(**params)\n",
    "        # score pipeline\n",
    "        score = -cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv).mean()\n",
    "        # return result\n",
    "        return {'loss': score, 'status': STATUS_OK}\n",
    "    \n",
    "    # hyperparametr tuning\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals)\n",
    "\n",
    "    # return \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    # set pipeline params\n",
    "    pipeline.set_params(**space_eval(space, best))\n",
    "    # score pipeline\n",
    "    score = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv)\n",
    "    res = {'best': space_eval(space, best), 'mean': score.mean(), 'std': score.std()}\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "    \n",
    "def process_all_pipelines(X, y, config, max_evals=30):\n",
    "\n",
    "    results = {}\n",
    "    for p_name, p_config in config.items():\n",
    "        # output message\n",
    "        print('\\nprocessing pipeline {}...'.format(p_name))\n",
    "        # defining columns to scale\n",
    "        cols = extract_column_names(X, p_config['col_filter'])\n",
    "        col_to_scale = extract_column_names(X[cols], '^(?!{})'.format('fe2__'))\n",
    "        # generate pipeline\n",
    "        pipeline = generate_pipeline(col_to_scale=col_to_scale, **p_config)\n",
    "        # hyperparameter tuning\n",
    "        results[p_name] = process_pipeline(X, y, pipeline, p_config['space'], max_evals=max_evals)\n",
    "    return results\n",
    "\n",
    "def stacking_predictions(X, y, X_test, pipeline_config, param_config):\n",
    "\n",
    "    estimators = []\n",
    "    for p_name, p_config in pipeline_config.items():\n",
    "        # defining columns to scale\n",
    "        cols = extract_column_names(X, p_config['col_filter'])\n",
    "        col_to_scale = extract_column_names(X[cols], '^(?!{})'.format('fe2__'))\n",
    "        # generate pipeline\n",
    "        pipeline = generate_pipeline(col_to_scale=col_to_scale, **p_config)\n",
    "        pipeline.set_params(**param_config[p_name]['best'])\n",
    "        estimator = ('{}'.format(p_name), pipeline)\n",
    "        estimators.append(estimator)\n",
    "    \n",
    "    # stacking with LogisticRegression as the meta learner\n",
    "    stacker = StackingClassifier(estimators=estimators,\n",
    "                             final_estimator=LogisticRegression())\n",
    "        \n",
    "    # score meta-learner\n",
    "#     cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "#     scores = cross_val_score(stacker, X, y, scoring='roc_auc', cv=cv)\n",
    "#     print('mean: {}, std: {}'.format(scores.mean(), scores.std()))\n",
    "    \n",
    "    # return predictions\n",
    "    preds = stacker.fit(X, y).predict_proba(X_test)[:, 1]\n",
    "    cols = ['Id', 'Choice']\n",
    "    X_test['Choice'] = preds\n",
    "    X_test['Id'] = X_test.index+1\n",
    "\n",
    "    return X_test.loc[:, cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T08:51:02.293397Z",
     "start_time": "2020-09-26T08:51:02.277495Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pipeline_config():\n",
    "    config = {}\n",
    "    #Define the hyperparameter configuration space for lgbm\n",
    "    space = {\n",
    "            'clf__n_estimators': hp.choice('clf__n_estimators', range(10, 101)),\n",
    "            'clf__max_depth': hp.choice('clf__max_depth', range(5, 51)),\n",
    "            'clf__min_child_weight': hp.quniform('clf__min_child_weight', 0.0 , 0.2, 0.01),\n",
    "            'clf__learning_rate': hp.quniform('clf__learning_rate', 0.005, 0.3, 0.01),\n",
    "            'clf__subsample': hp.quniform('clf__subsample', 0.1, 1.0, 0.05),\n",
    "            'clf__colsample_bylevel': hp.quniform('clf__colsample_bylevel', 0.1, 1.0, 0.05),\n",
    "            'clf__colsample_bytree': hp.quniform('clf__colsample_bytree', 0.1, 1.0, 0.05)\n",
    "            }\n",
    "    config['lgbm_1'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^fe0__',\n",
    "                      'power_tf': True,\n",
    "                      'scaler_tf': True,\n",
    "                      'space': space}\n",
    "\n",
    "    config['lgbm_2'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^(fe0__|fe2__)',\n",
    "                      'power_tf': True,\n",
    "                      'scaler_tf': True,\n",
    "                      'space': space}\n",
    "    config['lgbm_4'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^fe0__',\n",
    "                      'power_tf': False,\n",
    "                      'scaler_tf': False,\n",
    "                      'space': space}\n",
    "\n",
    "    config['lgbm_5'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^(fe0__|fe2__)',\n",
    "                      'power_tf': False,\n",
    "                      'scaler_tf': False,\n",
    "                      'space': space}\n",
    "\n",
    "    # Define the hyperparameter configuration space for xgboost\n",
    "    space = {\n",
    "            'clf__n_estimators': hp.choice('clf__n_estimators', range(10, 101)),\n",
    "            'clf__max_depth': hp.choice('clf__max_depth', range(5, 51)),\n",
    "            'clf__min_child_weight': hp.quniform('clf__min_child_weight', 0.0 , 0.2, 0.01),\n",
    "            'clf__learning_rate': hp.quniform('clf__learning_rate', 0.005, 0.3, 0.01),\n",
    "            'clf__subsample': hp.quniform('clf__subsample', 0.1, 1.0, 0.05),\n",
    "            'clf__colsample_bylevel': hp.quniform('clf__colsample_bylevel', 0.1, 1.0, 0.05),\n",
    "            'clf__colsample_bytree': hp.quniform('clf__colsample_bytree', 0.1, 1.0, 0.05),\n",
    "            'clf__gamma': hp.quniform('clf__gamma', 0., 0.5, 0.01),\n",
    "            }\n",
    "    config['xgboost_1'] = {'model': XGBClassifier(),\n",
    "                      'col_filter': '^fe0__',\n",
    "                      'power_tf': True,\n",
    "                      'scaler_tf': True,\n",
    "                      'space': space}\n",
    "\n",
    "    config['xgboost_2'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^(fe0__|fe2__)',\n",
    "                      'power_tf': True,\n",
    "                      'scaler_tf': True,\n",
    "                      'space': space}\n",
    "    config['xgboost_3'] = {'model': XGBClassifier(),\n",
    "                      'col_filter': '^fe0__',\n",
    "                      'power_tf': False,\n",
    "                      'scaler_tf': False,\n",
    "                      'space': space}\n",
    "\n",
    "    config['xgboost_4'] = {'model': LGBMClassifier(),\n",
    "                      'col_filter': '^(fe0__|fe2__)',\n",
    "                      'power_tf': False,\n",
    "                      'scaler_tf': False,\n",
    "                      'space': space}\n",
    "\n",
    "    # Define the hyperparameter configuration space for RF\n",
    "#     space = {\n",
    "#             'clf__n_estimators': hp.choice('clf__n_estimators', range(20, 205, 5)),\n",
    "#             'clf__max_depth': hp.choice('clf__max_depth', range(5, 51)),\n",
    "#             'clf__max_features' : hp.choice('clf__max_features', ['sqrt','log2',0.2,0.5,0.8]),\n",
    "#             'clf__criterion' : hp.choice('clf__criterion', ['gini','entropy']),    \n",
    "#             'clf__min_samples_leaf': hp.choice('clf__min_samples_leaf', range(1, 10)),\n",
    "#             'clf__min_samples_split': hp.choice('clf__min_samples_split', range(5, 20, 5))\n",
    "#             }\n",
    "#     config['RF_1'] = {'model': RandomForestClassifier(),\n",
    "#                       'col_filter': '^fe0__',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "#     config['RF_2'] = {'model': RandomForestClassifier(),\n",
    "#                       'col_filter': '^(fe0__|fe2__)',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "    # Define the hyperparameter configuration space for Adaboost   \n",
    "#     space = {\n",
    "#             'clf__n_estimators': hp.choice('clf__n_estimators', range(20, 205, 5)),\n",
    "#             'clf__learning_rate': hp.quniform('clf__learning_rate', 0.005, 0.3, 0.01),\n",
    "#             }\n",
    "#     config['Adaboost_1'] = {'model': AdaBoostClassifier(),\n",
    "#                       'col_filter': '^fe0__',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "#     config['Adaboost_2'] = {'model': AdaBoostClassifier(),\n",
    "#                       'col_filter': '^(fe0__|fe2__)',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "\n",
    "#     # Define the hyperparameter configuration space for extratree\n",
    "#     space = {\n",
    "#             'clf__n_estimators': hp.choice('clf__n_estimators', range(20, 205, 5)),\n",
    "#             'clf__max_depth': hp.choice('clf__max_depth', range(5, 51)),\n",
    "#             'clf__max_features' : hp.choice('clf__max_features', ['sqrt','log2',0.2,0.5,0.8]),\n",
    "#             'clf__criterion' : hp.choice('clf__criterion', ['gini','entropy']),    \n",
    "#             'clf__min_samples_leaf': hp.choice('clf__min_samples_leaf', range(1, 10)),\n",
    "#             'clf__min_samples_split': hp.choice('clf__min_samples_split', range(5, 20, 5))\n",
    "#             }\n",
    "#     config['extratree_1'] = {'model': ExtraTreesClassifier(),\n",
    "#                       'col_filter': '^fe0__',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "#     config['extratree_2'] = {'model': ExtraTreesClassifier(),\n",
    "#                       'col_filter': '^(fe0__|fe2__)',\n",
    "#                       'power_tf': True,\n",
    "#                       'scaler_tf': True,\n",
    "#                       'space': space}\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing all pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:03:15.259209Z",
     "start_time": "2020-09-26T08:51:20.012230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing pipeline lgbm_1...\n",
      "100%|██████████| 50/50 [01:37<00:00,  1.95s/trial, best loss: -0.8729717816649156]\n",
      "{'best': {'clf__colsample_bylevel': 0.4, 'clf__colsample_bytree': 0.15000000000000002, 'clf__learning_rate': 0.09, 'clf__max_depth': 13, 'clf__min_child_weight': 0.03, 'clf__n_estimators': 63, 'clf__subsample': 0.65}, 'mean': 0.8729717816649156, 'std': 0.008318871360400383}\n",
      "\n",
      "processing pipeline lgbm_2...\n",
      "100%|██████████| 50/50 [01:39<00:00,  2.00s/trial, best loss: -0.872760121840288] \n",
      "{'best': {'clf__colsample_bylevel': 1.0, 'clf__colsample_bytree': 0.2, 'clf__learning_rate': 0.1, 'clf__max_depth': 23, 'clf__min_child_weight': 0.0, 'clf__n_estimators': 52, 'clf__subsample': 0.35000000000000003}, 'mean': 0.872760121840288, 'std': 0.007306485761643223}\n",
      "\n",
      "processing pipeline lgbm_4...\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.92trial/s, best loss: -0.874581636672116] \n",
      "{'best': {'clf__colsample_bylevel': 0.45, 'clf__colsample_bytree': 0.30000000000000004, 'clf__learning_rate': 0.05, 'clf__max_depth': 50, 'clf__min_child_weight': 0.15, 'clf__n_estimators': 91, 'clf__subsample': 0.2}, 'mean': 0.874581636672116, 'std': 0.007295581901483406}\n",
      "\n",
      "processing pipeline lgbm_5...\n",
      "100%|██████████| 50/50 [00:30<00:00,  1.66trial/s, best loss: -0.8742006554917652]\n",
      "{'best': {'clf__colsample_bylevel': 0.55, 'clf__colsample_bytree': 0.35000000000000003, 'clf__learning_rate': 0.07, 'clf__max_depth': 30, 'clf__min_child_weight': 0.19, 'clf__n_estimators': 84, 'clf__subsample': 1.0}, 'mean': 0.8742006554917652, 'std': 0.008320666450851747}\n",
      "\n",
      "processing pipeline xgboost_1...\n",
      "100%|██████████| 50/50 [03:53<00:00,  4.66s/trial, best loss: -0.8712621110197982]\n",
      "{'best': {'clf__colsample_bylevel': 0.25, 'clf__colsample_bytree': 0.9, 'clf__gamma': 0.45, 'clf__learning_rate': 0.13, 'clf__max_depth': 5, 'clf__min_child_weight': 0.13, 'clf__n_estimators': 49, 'clf__subsample': 0.7000000000000001}, 'mean': 0.8712621110197982, 'std': 0.008238010170758857}\n",
      "\n",
      "processing pipeline xgboost_2...\n",
      "100%|██████████| 50/50 [01:43<00:00,  2.08s/trial, best loss: -0.8744493645254245]\n",
      "{'best': {'clf__colsample_bylevel': 0.55, 'clf__colsample_bytree': 0.25, 'clf__gamma': 0.44, 'clf__learning_rate': 0.07, 'clf__max_depth': 24, 'clf__min_child_weight': 0.18, 'clf__n_estimators': 75, 'clf__subsample': 0.5}, 'mean': 0.8744493645254245, 'std': 0.007398652879883707}\n",
      "\n",
      "processing pipeline xgboost_3...\n",
      "100%|██████████| 50/50 [01:26<00:00,  1.74s/trial, best loss: -0.8721728193343454]\n",
      "{'best': {'clf__colsample_bylevel': 0.35000000000000003, 'clf__colsample_bytree': 0.1, 'clf__gamma': 0.47000000000000003, 'clf__learning_rate': 0.04, 'clf__max_depth': 29, 'clf__min_child_weight': 0.02, 'clf__n_estimators': 68, 'clf__subsample': 0.9500000000000001}, 'mean': 0.8721728193343454, 'std': 0.0068301348108329015}\n",
      "\n",
      "processing pipeline xgboost_4...\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.86trial/s, best loss: -0.8738097809601342]\n",
      "{'best': {'clf__colsample_bylevel': 0.8, 'clf__colsample_bytree': 0.30000000000000004, 'clf__gamma': 0.21, 'clf__learning_rate': 0.08, 'clf__max_depth': 37, 'clf__min_child_weight': 0.12, 'clf__n_estimators': 65, 'clf__subsample': 0.15000000000000002}, 'mean': 0.8738097809601342, 'std': 0.008636759334907722}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/best-hyperparameters']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mode = 0\n",
    "\n",
    "# data prep\n",
    "df = pd.read_csv('data/train.csv').pipe(data_prep)\n",
    "X, y = df.drop(columns=TARGET_COL), df[TARGET_COL]\n",
    "if test_mode:\n",
    "    X, y = sampling(X,y,SAMPLE_SIZE, SEED)\n",
    "\n",
    "config = load_pipeline_config()\n",
    "res = process_all_pipelines(X, y, config, max_evals=50)\n",
    "\n",
    "# saving dict to file\n",
    "joblib.dump(res, 'data/best-hyperparameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scoring pipelines individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:08:05.182468Z",
     "start_time": "2020-09-26T09:08:00.675010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "p_names = ['lgbm_1', 'lgbm_2', 'lgbm_5', 'lgbm_4',\n",
    "           'xgboost_1', 'xgboost_2', 'xgboost_3', 'xgboost_4']\n",
    "# data_tf = False\n",
    "\n",
    "for p_name in p_names:\n",
    "    \n",
    "    # loading data + data prep\n",
    "    df = pd.read_csv('data/train.csv').pipe(data_prep)\n",
    "    X_test = pd.read_csv('data/test.csv').pipe(data_prep)\n",
    "    X, y = df.drop(columns=TARGET_COL), df[TARGET_COL]\n",
    "\n",
    "    # load pipeline config\n",
    "    pipeline_config = load_pipeline_config()\n",
    "    # load best parameters\n",
    "    param_config = joblib.load('data/best-hyperparameters')\n",
    "\n",
    "    # filter columns\n",
    "    regex = pipeline_config[p_name]['col_filter']\n",
    "    cols = extract_column_names(X, regex)\n",
    "    data_tf = pipeline_config[p_name]['scaler_tf']\n",
    "    X = X[cols]\n",
    "    X_test = X_test[cols]\n",
    "\n",
    "    # add transformer to pipeline definition\n",
    "    steps = []\n",
    "    if data_tf:\n",
    "        data_tf_str = 'with-tf'\n",
    "        scale_cols = extract_column_names(X, '^(?!{})'.format('fe2__'))\n",
    "        steps.append(('data_tf',\n",
    "                      ColumnTransformer(transformers=[\n",
    "                        ('power_tf', PowerTransformer(method='yeo-johnson', standardize=False), scale_cols),\n",
    "                        ('scaler_tf', StandardScaler(), scale_cols)\n",
    "                      ], remainder='passthrough')))\n",
    "\n",
    "    # add model to pipeline definition\n",
    "    steps.append(('clf', pipeline_config[p_name]['model']))\n",
    "\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.set_params(**param_config[p_name]['best'])\n",
    "\n",
    "    # generate preds\n",
    "    preds = pipeline.fit(X, y).predict_proba(X_test)[:, 1]\n",
    "    cols = ['Id', 'Choice']\n",
    "    X_test['Choice'] = preds\n",
    "    X_test['Id'] = X_test.index+1\n",
    "    X_test.loc[:, cols].to_csv('data/submissions/round2- preds-{}-{}.csv'.format(p_name, 'with-tf' if data_tf else 'no-tf'), index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:09:18.456516Z",
     "start_time": "2020-09-26T09:08:12.505239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-xgboost_3-no-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89.3k/89.3k [00:08<00:00, 10.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-lgbm_2-with-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:07<00:00, 19.6kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-xgboost_4-no-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:06<00:00, 21.1kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-lgbm_1-with-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:06<00:00, 22.3kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-xgboost_2-with-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:05<00:00, 25.3kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-lgbm_4-no-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:08<00:00, 17.3kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-lgbm_5-no-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:06<00:00, 22.1kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitting data/submissions/round2- preds-xgboost_1-with-tf.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90.1k/90.1k [00:05<00:00, 16.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# kaggle authentication\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# submist files in submission folder\n",
    "for file_path in glob.glob(\"data/submissions/*.csv\"):\n",
    "    print('submitting {}...'.format(file_path))\n",
    "    api.competition_submit(file_path,\n",
    "                           'API Submission',\n",
    "                           'predict-who-is-more-influential-in-a-social-network')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T10:46:37.677985Z",
     "start_time": "2020-09-25T10:46:37.664971Z"
    }
   },
   "source": [
    "## stacking only with best pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:09:37.996334Z",
     "start_time": "2020-09-26T09:09:37.850773Z"
    }
   },
   "outputs": [],
   "source": [
    "test_mode = 0\n",
    "\n",
    "df = pd.read_csv('data/train.csv').pipe(data_prep)\n",
    "X, y = df.drop(columns=TARGET_COL), df[TARGET_COL]\n",
    "if test_mode:\n",
    "    X, y = sampling(X,y,SAMPLE_SIZE, SEED)\n",
    "X_test = pd.read_csv('data/test.csv').pipe(data_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:16:15.231645Z",
     "start_time": "2020-09-26T09:16:01.623841Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pipeline config\n",
    "pipeline_config = load_pipeline_config()\n",
    "# restrict to best pipelines\n",
    "# best_pipelines = ['lgbm_1', 'lgbm_2', 'xgboost_1', 'xgboost_2', 'extratree_1']\n",
    "# best_pipelines = ['lgbm_2', 'xgboost_2']\n",
    "\n",
    "best_pipelines = ['lgbm_1', 'lgbm_2', 'lgbm_5', 'lgbm_4',\n",
    "                  'xgboost_1', 'xgboost_2', 'xgboost_3', 'xgboost_4']\n",
    "\n",
    "best_pipelines = dict(filter(lambda x: x[0] in best_pipelines, pipeline_config.items()))\n",
    "\n",
    "# load best parameters\n",
    "param_config = joblib.load('data/best-hyperparameters')\n",
    "\n",
    "res = stacking_predictions(X, y, X_test, best_pipelines, param_config)\n",
    "res.to_csv('data/preds-stacking-with-best-pipelines.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kaggle submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:16:26.702960Z",
     "start_time": "2020-09-26T09:16:17.655404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140k/140k [00:07<00:00, 19.4kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# kaggle authentication\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# submist files in submission folder\n",
    "api.competition_submit('data/preds-stacking-with-best-pipelines.csv',\n",
    "                       'API Submission',\n",
    "                       'predict-who-is-more-influential-in-a-social-network')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final score\n",
    "\n",
    "&emsp;. private Score: <b>0.87103</b> (16th place)<br>\n",
    "&emsp;. public Score: <b>0.87079</b> (10th place)<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
